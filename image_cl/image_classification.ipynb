{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn import model_selection\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_df, te_df = model_selection.train_test_split(df, test_size=.2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_im = tr_df['image_names']\n",
    "te_im = te_df['image_names']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emergency = \"1\"\n",
    "not_emergency = \"0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessing:\n",
    "    def __init__(self, x, y, batch_size, train_path, valid_path):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.batch_size = batch_size\n",
    "        self.train_path = train_path\n",
    "        self.valid_path = valid_path\n",
    "    \n",
    "    def generator(self):\n",
    "        train_datagen = ImageDataGenerator(\n",
    "                                            rescale=1./255,\n",
    "                                            rotation_range=30,\n",
    "                                            width_shift_range=0.2,\n",
    "                                            height_shift_range=0.4,\n",
    "                                            brightness_range=None,\n",
    "                                            shear_range=0.2,\n",
    "                                            zoom_range=0.2,\n",
    "                                            channel_shift_range=0.4,\n",
    "                                            fill_mode=\"nearest\",\n",
    "                                            cval=0.4,\n",
    "                                            horizontal_flip=True,\n",
    "                                            vertical_flip=True\n",
    "                                           )\n",
    "        test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "        \n",
    "        train_generator = train_datagen.flow_from_directory(\n",
    "                                                            self.train_path,  # this is the target directory\n",
    "                                                            target_size=(self.x, self.y),  # all images will be resized to 150x150\n",
    "                                                            batch_size=self.batch_size,\n",
    "                                                            class_mode='categorical')\n",
    "\n",
    "        valid_generator = test_datagen.flow_from_directory(\n",
    "                                                            self.valid_path,\n",
    "                                                            target_size=(self.x, self.y),\n",
    "                                                            batch_size=self.batch_size,\n",
    "                                                            class_mode='categorical')\n",
    "        \n",
    "        return train_generator, valid_generator\n",
    "    \n",
    "class Model:\n",
    "    def __init__(self, train_generator, valid_generator, met, los, model_link, x, y):\n",
    "        self.train_generator = train_generator\n",
    "        self.valid_generator = valid_generator\n",
    "        self.met = met\n",
    "        self.los = los\n",
    "        self.model_link = model_link\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "    \n",
    "    def compiler(self, activation, dense1, dropout):\n",
    "        self.dense = dense1\n",
    "        self.dropout = dropout\n",
    "        self.activation = activation\n",
    "        tl_model = tf.keras.Sequential([\n",
    "                    hub.KerasLayer(self.model_link, trainable=False),\n",
    "                    tf.keras.layers.Dropout(self.dropout),\n",
    "                    tf.keras.layers.Dense(self.dense, activation='relu'),\n",
    "                    tf.keras.layers.Dense(self.train_generator.num_classes, activation=self.activation)\n",
    "                ])\n",
    "        tl_model.build([None, self.x, self.y, 3])\n",
    "        \n",
    "        optimizer = tf.keras.optimizers.Adam(lr=1e-3)\n",
    "        tl_model.compile(optimizer=optimizer, loss=self.los, metrics=self.met)\n",
    "        \n",
    "        return tl_model\n",
    "    \n",
    "    def train(self, epochs, model):\n",
    "        self.epochs = epochs\n",
    "        self.model = model\n",
    "        steps_per_epoch = np.ceil(self.train_generator.samples/self.train_generator.batch_size)\n",
    "        val_steps_per_epoch = np.ceil(self.valid_generator.samples/self.valid_generator.batch_size)\n",
    "        hist = self.model.fit(\n",
    "                            self.train_generator, \n",
    "                            epochs=self.epochs,\n",
    "                            verbose=1,\n",
    "                            steps_per_epoch=steps_per_epoch,\n",
    "                            validation_data=self.valid_generator,\n",
    "                            validation_steps=val_steps_per_epoch).history\n",
    "\n",
    "        return self.model, hist\n",
    "    \n",
    "met = tf.keras.metrics.Accuracy()\n",
    "los = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "model_weights = 'https://tfhub.dev/google/imagenet/mobilenet_v2_075_224/classification/4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1316 images belonging to 2 classes.\n",
      "Found 330 images belonging to 2 classes.\n",
      "Epoch 1/50\n",
      "83/83 [==============================] - 21s 250ms/step - loss: 0.5970 - accuracy: 0.1877 - val_loss: 0.5545 - val_accuracy: 0.2803\n",
      "Epoch 2/50\n",
      "83/83 [==============================] - 20s 243ms/step - loss: 0.5783 - accuracy: 0.3214 - val_loss: 0.5453 - val_accuracy: 0.3318\n",
      "Epoch 3/50\n",
      "83/83 [==============================] - 20s 243ms/step - loss: 0.5781 - accuracy: 0.3974 - val_loss: 0.5439 - val_accuracy: 0.3667\n",
      "Epoch 4/50\n",
      "83/83 [==============================] - 20s 243ms/step - loss: 0.5763 - accuracy: 0.4309 - val_loss: 0.5534 - val_accuracy: 0.3758\n",
      "Epoch 5/50\n",
      "83/83 [==============================] - 20s 244ms/step - loss: 0.5687 - accuracy: 0.5315 - val_loss: 0.5474 - val_accuracy: 0.4712\n",
      "Epoch 6/50\n",
      "83/83 [==============================] - 21s 247ms/step - loss: 0.5698 - accuracy: 0.5976 - val_loss: 0.5560 - val_accuracy: 0.5636\n",
      "Epoch 7/50\n",
      "83/83 [==============================] - 20s 240ms/step - loss: 0.5734 - accuracy: 0.6364 - val_loss: 0.5498 - val_accuracy: 0.5045\n",
      "Epoch 8/50\n",
      "83/83 [==============================] - 20s 241ms/step - loss: 0.5729 - accuracy: 0.6269 - val_loss: 0.5440 - val_accuracy: 0.5394\n",
      "Epoch 9/50\n",
      "83/83 [==============================] - 20s 241ms/step - loss: 0.5730 - accuracy: 0.6166 - val_loss: 0.5470 - val_accuracy: 0.5303\n",
      "Epoch 10/50\n",
      "83/83 [==============================] - 21s 248ms/step - loss: 0.5753 - accuracy: 0.6467 - val_loss: 0.5490 - val_accuracy: 0.6076\n",
      "Epoch 11/50\n",
      "83/83 [==============================] - 20s 243ms/step - loss: 0.5762 - accuracy: 0.6919 - val_loss: 0.5443 - val_accuracy: 0.6485\n",
      "Epoch 12/50\n",
      "83/83 [==============================] - 20s 241ms/step - loss: 0.5724 - accuracy: 0.7109 - val_loss: 0.5557 - val_accuracy: 0.6818\n",
      "Epoch 13/50\n",
      "83/83 [==============================] - 20s 240ms/step - loss: 0.5693 - accuracy: 0.7318 - val_loss: 0.5485 - val_accuracy: 0.6939\n",
      "Epoch 14/50\n",
      "83/83 [==============================] - 21s 248ms/step - loss: 0.5714 - accuracy: 0.7325 - val_loss: 0.5498 - val_accuracy: 0.7258\n",
      "Epoch 15/50\n",
      "83/83 [==============================] - 20s 242ms/step - loss: 0.5741 - accuracy: 0.7356 - val_loss: 0.5468 - val_accuracy: 0.7485\n",
      "Epoch 16/50\n",
      "83/83 [==============================] - 20s 242ms/step - loss: 0.5692 - accuracy: 0.7348 - val_loss: 0.5548 - val_accuracy: 0.7439\n",
      "Epoch 17/50\n",
      "83/83 [==============================] - 20s 243ms/step - loss: 0.5707 - accuracy: 0.7553 - val_loss: 0.5540 - val_accuracy: 0.7530\n",
      "Epoch 18/50\n",
      "83/83 [==============================] - 20s 244ms/step - loss: 0.5663 - accuracy: 0.7762 - val_loss: 0.5521 - val_accuracy: 0.7848\n",
      "Epoch 19/50\n",
      "83/83 [==============================] - 20s 242ms/step - loss: 0.5697 - accuracy: 0.7770 - val_loss: 0.5622 - val_accuracy: 0.7424\n",
      "Epoch 20/50\n",
      "83/83 [==============================] - 20s 242ms/step - loss: 0.5773 - accuracy: 0.7675 - val_loss: 0.5526 - val_accuracy: 0.7712\n",
      "Epoch 21/50\n",
      "83/83 [==============================] - 20s 243ms/step - loss: 0.5665 - accuracy: 0.7853 - val_loss: 0.5543 - val_accuracy: 0.7803\n",
      "Epoch 22/50\n",
      "83/83 [==============================] - 20s 243ms/step - loss: 0.5757 - accuracy: 0.7481 - val_loss: 0.5534 - val_accuracy: 0.7773\n",
      "Epoch 23/50\n",
      "83/83 [==============================] - 21s 247ms/step - loss: 0.5775 - accuracy: 0.7610 - val_loss: 0.5564 - val_accuracy: 0.7939\n",
      "Epoch 24/50\n",
      "83/83 [==============================] - 20s 242ms/step - loss: 0.5751 - accuracy: 0.7739 - val_loss: 0.5576 - val_accuracy: 0.7939\n",
      "Epoch 25/50\n",
      "83/83 [==============================] - 20s 242ms/step - loss: 0.5719 - accuracy: 0.7831 - val_loss: 0.5450 - val_accuracy: 0.8167\n",
      "Epoch 26/50\n",
      "83/83 [==============================] - 20s 243ms/step - loss: 0.5702 - accuracy: 0.7812 - val_loss: 0.5501 - val_accuracy: 0.8197\n",
      "Epoch 27/50\n",
      "83/83 [==============================] - 20s 241ms/step - loss: 0.5635 - accuracy: 0.8131 - val_loss: 0.5485 - val_accuracy: 0.8258\n",
      "Epoch 28/50\n",
      "83/83 [==============================] - 20s 245ms/step - loss: 0.5666 - accuracy: 0.8055 - val_loss: 0.5511 - val_accuracy: 0.8197\n",
      "Epoch 29/50\n",
      "83/83 [==============================] - 20s 242ms/step - loss: 0.5697 - accuracy: 0.8021 - val_loss: 0.5491 - val_accuracy: 0.8288\n",
      "Epoch 30/50\n",
      "83/83 [==============================] - 20s 241ms/step - loss: 0.5684 - accuracy: 0.8078 - val_loss: 0.5489 - val_accuracy: 0.8333\n",
      "Epoch 31/50\n",
      "83/83 [==============================] - 20s 243ms/step - loss: 0.5666 - accuracy: 0.8021 - val_loss: 0.5496 - val_accuracy: 0.8273\n",
      "Epoch 32/50\n",
      "83/83 [==============================] - 21s 250ms/step - loss: 0.5701 - accuracy: 0.7986 - val_loss: 0.5519 - val_accuracy: 0.8167\n",
      "Epoch 33/50\n",
      "83/83 [==============================] - 20s 241ms/step - loss: 0.5636 - accuracy: 0.8131 - val_loss: 0.5475 - val_accuracy: 0.8348\n",
      "Epoch 34/50\n",
      "83/83 [==============================] - 20s 242ms/step - loss: 0.5693 - accuracy: 0.8013 - val_loss: 0.5494 - val_accuracy: 0.8227\n",
      "Epoch 35/50\n",
      "83/83 [==============================] - 20s 241ms/step - loss: 0.5735 - accuracy: 0.7994 - val_loss: 0.5491 - val_accuracy: 0.8242\n",
      "Epoch 36/50\n",
      "83/83 [==============================] - 20s 242ms/step - loss: 0.5686 - accuracy: 0.8021 - val_loss: 0.5512 - val_accuracy: 0.8136\n",
      "Epoch 37/50\n",
      "83/83 [==============================] - 20s 243ms/step - loss: 0.5775 - accuracy: 0.7910 - val_loss: 0.5513 - val_accuracy: 0.8106\n",
      "Epoch 38/50\n",
      "83/83 [==============================] - 20s 240ms/step - loss: 0.5670 - accuracy: 0.8097 - val_loss: 0.5479 - val_accuracy: 0.8455\n",
      "Epoch 39/50\n",
      "83/83 [==============================] - 20s 241ms/step - loss: 0.5687 - accuracy: 0.8248 - val_loss: 0.5483 - val_accuracy: 0.8455\n",
      "Epoch 40/50\n",
      "83/83 [==============================] - 20s 242ms/step - loss: 0.5704 - accuracy: 0.8169 - val_loss: 0.5491 - val_accuracy: 0.8439\n",
      "Epoch 41/50\n",
      "83/83 [==============================] - 21s 249ms/step - loss: 0.5708 - accuracy: 0.8176 - val_loss: 0.5477 - val_accuracy: 0.8652\n",
      "Epoch 42/50\n",
      "83/83 [==============================] - 20s 243ms/step - loss: 0.5733 - accuracy: 0.8104 - val_loss: 0.5477 - val_accuracy: 0.8652\n",
      "Epoch 43/50\n",
      "83/83 [==============================] - 20s 243ms/step - loss: 0.5647 - accuracy: 0.8298 - val_loss: 0.5462 - val_accuracy: 0.8576\n",
      "Epoch 44/50\n",
      "83/83 [==============================] - 20s 241ms/step - loss: 0.5657 - accuracy: 0.8203 - val_loss: 0.5461 - val_accuracy: 0.8561\n",
      "Epoch 45/50\n",
      "83/83 [==============================] - 20s 242ms/step - loss: 0.5644 - accuracy: 0.8271 - val_loss: 0.5492 - val_accuracy: 0.8576\n",
      "Epoch 46/50\n",
      "83/83 [==============================] - 20s 244ms/step - loss: 0.5643 - accuracy: 0.8203 - val_loss: 0.5468 - val_accuracy: 0.8652\n",
      "Epoch 47/50\n",
      "83/83 [==============================] - 20s 242ms/step - loss: 0.5689 - accuracy: 0.8062 - val_loss: 0.5468 - val_accuracy: 0.8652\n",
      "Epoch 48/50\n",
      "83/83 [==============================] - 20s 241ms/step - loss: 0.5679 - accuracy: 0.8169 - val_loss: 0.5490 - val_accuracy: 0.8591\n",
      "Epoch 49/50\n",
      "83/83 [==============================] - 21s 248ms/step - loss: 0.5645 - accuracy: 0.8267 - val_loss: 0.5470 - val_accuracy: 0.8742\n",
      "Epoch 50/50\n",
      "83/83 [==============================] - 20s 243ms/step - loss: 0.5653 - accuracy: 0.8309 - val_loss: 0.5481 - val_accuracy: 0.8561\n"
     ]
    }
   ],
   "source": [
    "preprocess = Preprocessing(224, 224, 16, 'train/', 'valid/')\n",
    "train_generator, valid_generator = Preprocessing.generator(preprocess)\n",
    "\n",
    "image_class_model = Model(train_generator, valid_generator, met, los, model_weights, 224, 224)\n",
    "tl_model = Model.compiler(image_class_model, 'sigmoid', 2560, 0.5)\n",
    "img_model, img_hist = Model.train(image_class_model, 50, tl_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Predictions:\n",
    "    \n",
    "    def __init__(self, image_dir, model, test_df, x, y):\n",
    "        self.image_dir = image_dir\n",
    "        self.model = model\n",
    "        self.test_df = test_df\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "    \n",
    "    def predict(self, image_name):\n",
    "        self.image_name = image_name\n",
    "        test_img = cv2.imread(os.path.join(self.image_dir, self.image_name))\n",
    "        test_img = np.resize(test_img, (1, self.x, self.y, 3))\n",
    "        tf_model_predictions = self.model.predict(test_img)\n",
    "        id_ = np.argmax(tf_model_predictions)\n",
    "        \n",
    "        return image_name, id_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1795.jpg 0\n"
     ]
    }
   ],
   "source": [
    "prediction_module = Predictions('images/', img_model, test, 224, 224)\n",
    "name, label = Predictions.predict(prediction_module, test['image_names'].iloc[47])\n",
    "print(name, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
